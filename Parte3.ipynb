{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3ro Entregable del Reto\n",
    "## Evaluación y Refinamiento de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold,RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de base de datos limpia \n",
    "Separación de features de label, estandarización de datos y división de datos entre Train set y Test set para su entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Titanic_train_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de datos entre Train(Entrenamiento) y Test(Prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain=df.iloc[:891]\n",
    "dftest=df.iloc[891:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest=dftest.drop(columns=['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dftrain['Survived']\n",
    "X=dftrain.drop(columns='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "Xf=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de los datos en train y test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Creación de Grid search para encontrar los mejores parámetros para el algoritmo Random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(clf_model, param_grid, name):\n",
    "\n",
    "    # Define the feature selector\n",
    "    rfecv_model = RFECV(estimator=clf_model, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "\n",
    "    # Creación de pipeline con el modelo de Random Forest y el selector de features\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', rfecv_model),\n",
    "        ('classification', clf_model)\n",
    "        ])\n",
    "    \n",
    "    # Define the GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=StratifiedKFold(10), scoring='roc_auc_ovr', n_jobs=-1)\n",
    "\n",
    "    # Hacerle fit model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and score\n",
    "    print(\"Best parameters found for \",name,\":\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score for \",name,\":\", grid_search.best_score_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found for  Random Forest : {'classification__criterion': 'gini', 'classification__max_depth': 10, 'classification__max_features': 'sqrt', 'classification__max_leaf_nodes': 10, 'classification__n_estimators': 200}\n",
      "Best cross-validation score for  Random Forest : 0.8513247863247864\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier\n",
    "clf_rf = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classification__n_estimators': [ 5, 10, 50, 200, 250],\n",
    "    'classification__max_features': ['sqrt','log2'],\n",
    "    'classification__max_depth': [4, 6, 8, 10, 12],\n",
    "    'classification__criterion':['gini','log_loss','entropy'],\n",
    "    'classification__max_leaf_nodes':[2, 5, 10]\n",
    "}\n",
    "\n",
    "gridSearch(clf_rf, param_grid_rf, name = \"Random Forest\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbd = RandomForestClassifier(\n",
    "    criterion ='gini',\n",
    "    max_depth = 6,\n",
    "    max_features = 'sqrt',\n",
    "    max_leaf_nodes = 20,\n",
    "    class_weight='balanced',\n",
    "    n_estimators = 200\n",
    ")\n",
    "\n",
    "rbd.fit(X_train, y_train)\n",
    "\n",
    "y_predRF = rbd.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_predRF)\n",
    "f1 = f1_score(y_test, y_predRF)\n",
    "roc_auc = roc_auc_score(y_test, y_predRF)\n",
    "conf_matrix = confusion_matrix(y_test, y_predRF)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC-AUC: {roc_auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found for  XGBoost : {'classification__learning_rate': 0.001, 'classification__max_depth': 8, 'classification__n_estimators': 200}\n",
      "Best cross-validation score for  XGBoost : 0.8379390773835217\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier\n",
    "clf_xgb = xgb.XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'classification__n_estimators': [200,600],\n",
    "    'classification__max_depth': [4,6,8,10,12],\n",
    "    'classification__learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "gridSearch(clf_xgb, param_grid_xgb, name = \"XGBoost\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8100558659217877\n",
      "F1 Score: 0.7671232876712328\n",
      "ROC-AUC: 0.8021879021879021\n",
      "Confusion Matrix:\n",
      "[[89 16]\n",
      " [18 56]]\n"
     ]
    }
   ],
   "source": [
    "rbd = xgb.XGBClassifier(\n",
    "    criterion ='gini',\n",
    "    max_depth = 6,\n",
    "    max_features = 'sqrt',\n",
    "    max_leaf_nodes = 20,\n",
    "    class_weight='balanced',\n",
    "    n_estimators = 200\n",
    ")\n",
    "\n",
    "rbd.fit(X_train, y_train)\n",
    "\n",
    "y_predXGB = rbd.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_predXGB)\n",
    "f1 = f1_score(y_test, y_predXGB)\n",
    "roc_auc = roc_auc_score(y_test, y_predXGB)\n",
    "conf_matrix = confusion_matrix(y_test, y_predXGB)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC-AUC: {roc_auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found for  Logistic Regression : {'classification__fit_intercept': False, 'classification__intercept_scaling': 1, 'classification__max_iter': 200, 'classification__penalty': 'l2', 'classification__solver': 'liblinear'}\n",
      "Best cross-validation score for  Logistic Regression : 0.8354256957034736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "1280 fits failed out of a total of 2560.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1182, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "114 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "123 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "198 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "76 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "79 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.83533893 0.83533893 0.83500727 0.83500727        nan        nan\n",
      "        nan        nan 0.83533893 0.83533893 0.83500727 0.83500727\n",
      "        nan        nan        nan        nan 0.83533893 0.83533893\n",
      " 0.83500727 0.83500727        nan        nan        nan        nan\n",
      " 0.83533893 0.83533893 0.83500727 0.83500727        nan        nan\n",
      "        nan        nan 0.83533893 0.83533893 0.83500727 0.83500727\n",
      "        nan        nan        nan        nan 0.83533893 0.83533893\n",
      " 0.83500727 0.83500727        nan        nan        nan        nan\n",
      " 0.83533893 0.83533893 0.83500727 0.83500727        nan        nan\n",
      "        nan        nan 0.83533893 0.83533893 0.83500727 0.83500727\n",
      "        nan        nan        nan        nan 0.83533893 0.83533893\n",
      " 0.83509144 0.83500727        nan        nan        nan        nan\n",
      " 0.83533893 0.83533893 0.83509144 0.83500727        nan        nan\n",
      "        nan        nan 0.83533893 0.83533893 0.83509144 0.83500727\n",
      "        nan        nan        nan        nan 0.83533893 0.83533893\n",
      " 0.83509144 0.83500727        nan        nan        nan        nan\n",
      " 0.83533893 0.83533893 0.83500727 0.83500727        nan        nan\n",
      "        nan        nan 0.83533893 0.83533893 0.83500727 0.83500727\n",
      "        nan        nan        nan        nan 0.83533893 0.83533893\n",
      " 0.83500727 0.83500727        nan        nan        nan        nan\n",
      " 0.83533893 0.83533893 0.83500727 0.83500727        nan        nan\n",
      "        nan        nan 0.83542124 0.83542124 0.8354257  0.8354257\n",
      "        nan        nan        nan        nan 0.83542124 0.83542124\n",
      " 0.8354257  0.8354257         nan        nan        nan        nan\n",
      " 0.83542124 0.83542124 0.8354257  0.8354257         nan        nan\n",
      "        nan        nan 0.83542124 0.83542124 0.8354257  0.8354257\n",
      "        nan        nan        nan        nan 0.83542124 0.83542124\n",
      " 0.8354257  0.8354257         nan        nan        nan        nan\n",
      " 0.83542124 0.83542124 0.8354257  0.8354257         nan        nan\n",
      "        nan        nan 0.83542124 0.83542124 0.8354257  0.8354257\n",
      "        nan        nan        nan        nan 0.83542124 0.83542124\n",
      " 0.8354257  0.8354257         nan        nan        nan        nan\n",
      " 0.83542124 0.83542124 0.8354257  0.8354257         nan        nan\n",
      "        nan        nan 0.83542124 0.83542124 0.8354257  0.8354257\n",
      "        nan        nan        nan        nan 0.83542124 0.83542124\n",
      " 0.8354257  0.8354257         nan        nan        nan        nan\n",
      " 0.83542124 0.83542124 0.8354257  0.8354257         nan        nan\n",
      "        nan        nan 0.83542124 0.83542124 0.8354257  0.8354257\n",
      "        nan        nan        nan        nan 0.83542124 0.83542124\n",
      " 0.8354257  0.8354257         nan        nan        nan        nan\n",
      " 0.83542124 0.83542124 0.8354257  0.8354257         nan        nan\n",
      "        nan        nan 0.83542124 0.83542124 0.8354257  0.8354257\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the classifier\n",
    "clf_rl = LogisticRegression(random_state=42, class_weight=\"balanced\", solver=\"liblinear\")\n",
    "\n",
    "\n",
    "param_grid_rl = {\n",
    "    'classification__max_iter': [200, 250, 300, 500],\n",
    "    'classification__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'classification__solver': ['liblinear', 'saga'],  # Match solvers to penalties\n",
    "    'classification__intercept_scaling': [1, 2, 5, 10],\n",
    "    'classification__fit_intercept': [True, False]\n",
    "    }\n",
    "\n",
    "gridSearch(clf_rl, param_grid_rl, name = \"Logistic Regression\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8156424581005587\n",
      "F1 Score: 0.7898089171974523\n",
      "ROC-AUC: 0.818918918918919\n",
      "Confusion Matrix:\n",
      "[[84 21]\n",
      " [12 62]]\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(\n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    solver='liblinear', \n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "lgr.fit(X_train, y_train)\n",
    "\n",
    "y_predLR = lgr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_predLR)\n",
    "f1 = f1_score(y_test, y_predLR)\n",
    "roc_auc = roc_auc_score(y_test, y_predLR)\n",
    "conf_matrix = confusion_matrix(y_test, y_predLR)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC-AUC: {roc_auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1209, 'min_samples_split': 6, 'max_features': 'sqrt', 'max_depth': 4, 'criterion': 'gini', 'bootstrap': True}\n",
      "Accuracy: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "rfc_search_space = {\n",
    "    'n_estimators': range(200, 1600),\n",
    "    'criterion':['gini','log_loss','entropy'],\n",
    "    'max_depth': range(2, 51),\n",
    "    'min_samples_split': range(2, 11),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=clf_rf, param_distributions=rfc_search_space, n_iter=200, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "rfc = RandomForestClassifier(**best_params)\n",
    "print(best_params)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "\n",
    "xgb_search_space = {\n",
    "    'n_estimators': range(200, 1600),\n",
    "    'max_depth': range(2, 51),\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=clf_xgb, param_distributions=xgb_search_space, n_iter=200, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "xgbc = xgb.XGBClassifier(**best_params)\n",
    "print(best_params)\n",
    "\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_pred = xgbc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aoran\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 450 is smaller than n_iter=500. Running 450 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_iter': 50}\n",
      "Accuracy: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "clf_lgr = LogisticRegression(random_state=42, class_weight=\"balanced\", solver=\"liblinear\")\n",
    "\n",
    "lgr_search_space = {\n",
    "    'max_iter': range(50, 500)\n",
    "}\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=clf_lgr, param_distributions=lgr_search_space, n_iter=500, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "lgrc = LogisticRegression(**best_params)\n",
    "print(best_params)\n",
    "\n",
    "lgrc.fit(X_train, y_train)\n",
    "y_pred = lgrc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo que estaba comentado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''rf = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10, random_state=30)\n",
    "rf.fit(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''y_predRF = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_predRF)\n",
    "f1 = f1_score(y_test, y_predRF)\n",
    "roc_auc = roc_auc_score(y_test, y_predRF)\n",
    "conf_matrix = confusion_matrix(y_test, y_predRF)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC-AUC: {roc_auc}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
